{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sample-solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tqtg/is712-2019/blob/master/sample_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCu5pNL4YkQ4",
        "colab_type": "text"
      },
      "source": [
        "*Licensed under the Apache 2.0 License.*\n",
        "\n",
        "# IS712-2019 Course Project Sample Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZsvkFrMH6EC",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We use Google Drive to store data, model checkpoints, etc. Thus, we need to mount it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwPqtrtPYqZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbDOUrsQJDhY",
        "colab_type": "text"
      },
      "source": [
        "Unzip data from Google Drive into local runtime host:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdpKMkueQpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq /content/drive/My\\ Drive/IS712/data/train.zip -d ./data\n",
        "!unzip -qq /content/drive/My\\ Drive/IS712/data/public_test.zip -d ./data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxfhuD1NJhzG",
        "colab_type": "text"
      },
      "source": [
        "Make sure GPU is available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7YdvAbiYWMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AYuYJd2Tmqb",
        "colab_type": "text"
      },
      "source": [
        "## Global Settings and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31fsahC2TlUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from datetime import datetime\n",
        "from itertools import combinations\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tnrange\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "random.seed(712)\n",
        "np.random.seed(712)\n",
        "\n",
        "print(\"Numpy version:\", np.__version__)\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk9S0tIDY2JT",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "Given the data, we split into training and test sets for model validation purpose. \n",
        "\n",
        "Datasets are constructed by sampling. For each entity, all combinations of image pairs from the same entity will serve as positive examples. For each positive pair, a corresponding negative pair will be sampled by two steps (i.e., negative entity then negative image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCAaKOvPUPe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"./data/train\"\n",
        "TEST_SIZE = 0.5\n",
        "IMG_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQFNtDqoah6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### DATA SAMPLING ###\n",
        "\n",
        "def sample(folder_img_paths):\n",
        "    data = []\n",
        "    for folder, img_paths in folder_img_paths.items():\n",
        "        # positive pairs\n",
        "        pos_pairs = [(p1, p2) for (p1, p2) in combinations(img_paths, 2)]\n",
        "\n",
        "        # negative pairs\n",
        "        neg_pairs = []\n",
        "        for _ in range(len(pos_pairs)):\n",
        "            img_path1 = np.random.choice(img_paths, 1)[0]\n",
        "            # sample negative folder\n",
        "            neg_folders = [f for f in folder_img_paths.keys() if f != folder]\n",
        "            assert len(neg_folders) == len(folder_img_paths.keys()) - 1\n",
        "            neg_folder = np.random.choice(neg_folders, 1)[0]\n",
        "            # sample negative image\n",
        "            img_path2 = np.random.choice(folder_img_paths[neg_folder], 1)[0]\n",
        "            neg_pairs.append((img_path1, img_path2))\n",
        "\n",
        "        # combine positive and negative data\n",
        "        data.extend([(p1, p2, 1) for (p1, p2) in pos_pairs])\n",
        "        data.extend([(p1, p2, 0) for (p1, p2) in neg_pairs])\n",
        "\n",
        "    random.shuffle(data)\n",
        "    return np.asarray(data)\n",
        "\n",
        "\n",
        "def gen_data():\n",
        "    train_img_paths = {}\n",
        "    test_img_paths = {}\n",
        "    for folder in glob.glob(DATA_DIR + '/*/'):\n",
        "        img_paths = [p for p in glob.glob(folder + '/*.jpg')]\n",
        "        random.shuffle(img_paths)\n",
        "        n_train = int(len(img_paths) * (1 - TEST_SIZE))\n",
        "        train_img_paths[folder] = img_paths[:n_train]\n",
        "        test_img_paths[folder] = img_paths[n_train:]\n",
        "\n",
        "    train_data = sample(train_img_paths)\n",
        "    test_data = sample(test_img_paths)\n",
        "    print('Training size: {}'.format(len(train_data)))\n",
        "    print('Test size: {}'.format(len(test_data)))\n",
        "    return train_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxzeFidbaq6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### IMAGE READING PARSING ###\n",
        "\n",
        "def read_img(img_path, is_training=False):\n",
        "    img_string = tf.read_file(img_path)\n",
        "    img_decoded = tf.image.decode_jpeg(img_string, channels=3)\n",
        "    img = tf.image.resize(img_decoded, [IMG_SIZE, IMG_SIZE])\n",
        "    img = img / 255.0\n",
        "\n",
        "    if is_training:\n",
        "        \"\"\"Data augmentation comes here\"\"\"\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def parse_function(img_path1, img_path2, label, is_training=False):\n",
        "    img1 = read_img(img_path1, is_training)\n",
        "    img2 = read_img(img_path2, is_training)\n",
        "    return img1, img2, [label]\n",
        "\n",
        "\n",
        "def parse_function_train(img_path1, img_path2, label):\n",
        "    return parse_function(img_path1, img_path2, label, is_training=True)\n",
        "\n",
        "\n",
        "def parse_function_test(img_path1, img_path2, label):\n",
        "    return parse_function(img_path1, img_path2, label, is_training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCPSRkUoWqnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### DATA SERVING ###\n",
        "\n",
        "class DataGenerator(object):\n",
        "\n",
        "    def __init__(self, batch_size=1, num_threads=1, \n",
        "                 train_shuffle=False, buffer_size=10000):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_threads = num_threads\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "        # data sampling and spliting\n",
        "        self.train_data, self.test_data = gen_data()\n",
        "\n",
        "        # build iterator\n",
        "        self.train_set = self._build_data_set(self.train_data, \n",
        "                                              parse_function_train, \n",
        "                                              shuffle=train_shuffle)\n",
        "        self.iterator = tf.data.Iterator.from_structure(self.train_set.output_types,\n",
        "                                                        self.train_set.output_shapes)\n",
        "        # for training\n",
        "        self.train_init_op = self.iterator.make_initializer(self.train_set)\n",
        "        self.next = self.iterator.get_next()\n",
        "        self.num_train_batches = int(np.ceil(len(self.train_data) / batch_size))\n",
        "        # for testing\n",
        "        self.test_set = self._build_data_set(self.test_data, parse_function_test)\n",
        "        self.test_init_op = self.iterator.make_initializer(self.test_set)\n",
        "        self.num_test_batches = int(np.ceil(len(self.test_data) / batch_size))\n",
        "\n",
        "    def _build_data_set(self, data, map_fn, shuffle=False):\n",
        "        \"\"\"\n",
        "        Images are loaded from disk and processed batch by batch. Since our dataset\n",
        "        is not that big, it would be faster if we load all the images into RAM once \n",
        "        and read from their. I leave it for you guys to explore :)\n",
        "        \"\"\"\n",
        "        img_path1 = tf.convert_to_tensor(data[:, 0], dtype=tf.string)\n",
        "        img_path2 = tf.convert_to_tensor(data[:, 1], dtype=tf.string)\n",
        "        labels = tf.convert_to_tensor(data[:, 2], dtype=tf.int32)\n",
        "        data = tf.data.Dataset.from_tensor_slices((img_path1, img_path2, labels))\n",
        "        if shuffle:\n",
        "            data = data.shuffle(buffer_size=self.buffer_size)\n",
        "        data = data.map(map_fn, num_parallel_calls=self.num_threads)\n",
        "        data = data.batch(self.batch_size)\n",
        "        data = data.prefetch(self.num_threads)\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k01LO3s7ZK_y",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9XP2RmPZOaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(object):\n",
        "\n",
        "    def __init__(self, training=False):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, IMG_SIZE, IMG_SIZE, 3])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, IMG_SIZE, IMG_SIZE, 3])\n",
        "        self.y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "        net1 = self._encoder(self.x1)\n",
        "        net2 = self._encoder(self.x2)\n",
        "        net = tf.abs(net1 - net2)\n",
        "\n",
        "        with tf.variable_scope('classifier'):\n",
        "            self.logits = tf.layers.dense(net, 1, name='logits')\n",
        "            self.prob = tf.nn.sigmoid(self.logits, name='prob')\n",
        "\n",
        "        if training:\n",
        "            self.loss, self.train_op = self._loss_fn()\n",
        "\n",
        "    def _encoder(self, input, name='encoder'):\n",
        "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "            net = tf.layers.flatten(input)\n",
        "            net = tf.layers.dense(net, units=300, activation=tf.nn.relu)\n",
        "            net = tf.layers.dense(net, units=300, activation=tf.nn.relu)\n",
        "            return net\n",
        "\n",
        "    def _loss_fn(self):\n",
        "        trained_vars = tf.trainable_variables()\n",
        "\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, \n",
        "                                                                logits=self.logits)\n",
        "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
        "        l2_reg = tf.add_n([tf.nn.l2_loss(v) for v in trained_vars \n",
        "                           if 'bias' not in v.name])\n",
        "        loss = cross_entropy + LAMBDA_REG * l2_reg\n",
        "\n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE, \n",
        "                                           beta1=0.9, \n",
        "                                           beta2=0.99, \n",
        "                                           epsilon=1e-8)\n",
        "        train_op = optimizer.minimize(loss, global_step, var_list=trained_vars)\n",
        "\n",
        "        return loss, train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6HZa5DjYbMs",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g08jYs1rWbIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 0.001\n",
        "LAMBDA_REG = 0.0\n",
        "\n",
        "NUM_CHECKPOINTS = 5\n",
        "NUM_THREADS = 4\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/My Drive/IS712/checkpoints\"\n",
        "if tf.gfile.Exists(CHECKPOINT_DIR):\n",
        "    tf.gfile.DeleteRecursively(CHECKPOINT_DIR)\n",
        "tf.gfile.MakeDirs(CHECKPOINT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfd5xUdGaCAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = DataGenerator(batch_size=BATCH_SIZE, num_threads=NUM_THREADS, \n",
        "                          train_shuffle=True, buffer_size=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtpfN443buGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just a useful function for parameter counting \n",
        "def count_parameters(trained_vars):\n",
        "    total_parameters = 0\n",
        "    print('=' * 100)\n",
        "    for variable in trained_vars:\n",
        "        variable_parameters = 1\n",
        "        for dim in variable.get_shape():\n",
        "            variable_parameters *= dim.value\n",
        "        print('{:70} {:20} params'.format(variable.name, variable_parameters))\n",
        "        print('-' * 100)\n",
        "        total_parameters += variable_parameters\n",
        "    print('=' * 100)\n",
        "    print(\"Total trainable parameters: %d\" % total_parameters)\n",
        "    print('=' * 100)\n",
        "\n",
        "\n",
        "model = MLP(training=True)\n",
        "count_parameters(tf.trainable_variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZODfEitQXusI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=NUM_CHECKPOINTS)\n",
        "    \n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        print(\"\\n{} Epoch: {}/{}\".format(datetime.now(), epoch, NUM_EPOCHS))\n",
        "\n",
        "        # Training\n",
        "        sum_loss = 0.\n",
        "        sess.run(generator.train_init_op)\n",
        "        loop = tnrange(generator.num_train_batches, desc='Training')\n",
        "        for step in loop:\n",
        "            batch_img1, batch_img2, batch_label = sess.run(generator.next)\n",
        "            _, loss = sess.run([model.train_op, model.loss], \n",
        "                                feed_dict={model.x1: batch_img1,\n",
        "                                           model.x2: batch_img2,\n",
        "                                           model.y: batch_label})\n",
        "            sum_loss += loss\n",
        "            loop.set_postfix(loss=(sum_loss / (step + 1)))\n",
        "        print('Training loss: {:.6f}'.format(sum_loss))\n",
        "  \n",
        "        saver.save(sess, \n",
        "                   os.path.join(CHECKPOINT_DIR, 'model_e{}.ckpt'.format(epoch)))\n",
        "          \n",
        "        # Testing\n",
        "        pds = []\n",
        "        gts = []\n",
        "        sum_loss = 0.\n",
        "        sess.run(generator.test_init_op)\n",
        "        loop = tnrange(generator.num_test_batches, desc='Testing')\n",
        "        for step in loop:\n",
        "            batch_img1, batch_img2, batch_label = sess.run(generator.next)\n",
        "            prob, loss = sess.run([model.prob, model.loss],\n",
        "                                   feed_dict={model.x1: batch_img1,\n",
        "                                              model.x2: batch_img2,\n",
        "                                              model.y: batch_label})\n",
        "            sum_loss += loss\n",
        "            loop.set_postfix(loss=(sum_loss / (step + 1)))\n",
        "            pds.extend(np.round(prob).ravel().tolist())\n",
        "            gts.extend(batch_label.ravel().tolist())\n",
        "        pds = np.asarray(pds)\n",
        "        gts = np.asarray(gts)\n",
        "        print('Test loss: {:.6f}'.format(sum_loss))\n",
        "        print('Test acc: {:.6f}'.format(np.equal(pds, gts).sum() / len(gts)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRPvsb2UdNew",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJmvmS67eO77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"./data/public_test\"\n",
        "CHECKPOINT = \"/content/drive/My Drive/IS712/checkpoints/model_e{}.ckpt\".format(NUM_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oUviAOejpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_test_data(input_file):\n",
        "    test_data = []\n",
        "    with open(input_file, 'r') as f:\n",
        "        for line in f:\n",
        "            img1, img2 = line.strip().split('\\t')\n",
        "            test_data.append((os.path.join(DATA_DIR, 'images', img1),\n",
        "                              os.path.join(DATA_DIR, 'images', img2)))\n",
        "    return np.asarray(test_data)\n",
        "\n",
        "\n",
        "def data_generator(input_file):\n",
        "    test_data = read_test_data(input_file)\n",
        "    img_path1 = tf.convert_to_tensor(test_data[:, 0], dtype=tf.string)\n",
        "    img_path2 = tf.convert_to_tensor(test_data[:, 1], dtype=tf.string)\n",
        "\n",
        "    def parse_function(img_path1, img_path2):\n",
        "        img1 = read_img(img_path1, is_training=False)\n",
        "        img2 = read_img(img_path2, is_training=False)\n",
        "        return img1, img2\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((img_path1, img_path2))\n",
        "    test_set = test_set.map(parse_function, num_parallel_calls=NUM_THREADS)\n",
        "    test_set = test_set.batch(BATCH_SIZE)\n",
        "    test_set = test_set.prefetch(NUM_THREADS)\n",
        "\n",
        "    iterator = tf.data.Iterator.from_structure(test_set.output_types, test_set.output_shapes)\n",
        "    init_op = iterator.make_initializer(test_set)\n",
        "    next = iterator.get_next()\n",
        "    num_batches = int(np.ceil(len(test_data) / BATCH_SIZE))\n",
        "\n",
        "    return init_op, next, num_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmjIIWOhZGOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('./submission'):\n",
        "    os.makedirs('./submission')\n",
        "\n",
        "tf.reset_default_graph()\n",
        "model = MLP(training=False)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    tf.train.Saver().restore(sess, CHECKPOINT)\n",
        "\n",
        "    for input_file in glob.glob(os.path.join(DATA_DIR, 'pairs/*.txt')):\n",
        "        basename = os.path.basename(input_file)\n",
        "        init_op, next, num_batches = data_generator(input_file)\n",
        "        pds = []\n",
        "        sess.run(init_op)\n",
        "        for _ in tnrange(num_batches, desc=basename):\n",
        "            batch_img1, batch_img2 = sess.run(next)\n",
        "            prob = sess.run(model.prob, feed_dict={model.x1: batch_img1,\n",
        "                                                   model.x2: batch_img2})\n",
        "            pds.extend(np.round(prob).ravel().tolist())\n",
        "\n",
        "        # write prediction to submission files\n",
        "        with open('./submission/{}'.format(basename), 'w') as f:\n",
        "            f.write('\\n'.join(str(int(pd)) for pd in pds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_29IqK_q3dry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd submission; zip /content/drive/My\\ Drive/IS712/submission.zip *"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}